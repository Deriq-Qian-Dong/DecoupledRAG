training:
  model_name_or_path: meta-llama/Llama-2-7b-hf
  num_layers_unfrozen: 2
  optimizer: 
    name: Lamb
    kwargs:
      lr: 0.001
      weight_decay: 0.0
      betas: [0.9, 0.999]
      eps: 1e-08
  scheduler:
    name: CosineAnnealingLR
    kwargs:
      T_max: 1000
      eta_min: 0.0001
  batch_size: 4
  num_epochs: 10

dataset:
  train:
    data_name_or_path: Dahoas/rm-static
    max_seq_len: 512
    train_or_test: train
  test:
    data_name_or_path: Dahoas/rm-static
    max_seq_len: 512
    train_or_test: test
