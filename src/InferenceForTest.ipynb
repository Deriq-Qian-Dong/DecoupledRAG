{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ec3c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-01 01:01:05,575] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/root/paddlejob/workspace/env_run/ReGPT\n",
      "trainer: ReGPTLanguageModelTrainer\n",
      "training:\n",
      "\tproject_name: llama2-7b-sft\n",
      "\tmodel_name_or_path: output/SFT-best/\n",
      "\ttokenizer_name_or_path: ../data_of_ReGPT/llama2-7b-phrase-tokenizer-trained-on-WikiText103/\n",
      "\tnum_layers_unfrozen: -1\n",
      "\tgradient_checkpointing: True\n",
      "\toptimizer:\n",
      "\t\tname: Lamb\n",
      "\t\tkwargs:\n",
      "\t\t\tlr: 1e-4\n",
      "\t\t\tweight_decay: 0.0\n",
      "\t\t\tbetas: [0.9, 0.999]\n",
      "\t\t\teps: 1e-08\n",
      "\tscheduler:\n",
      "\t\tname: CosineAnnealingLR\n",
      "\t\tkwargs:\n",
      "\t\t\tT_max: 1000\n",
      "\t\t\teta_min: 3e-5\n",
      "\tnum_epochs: 1000\n",
      "\tlog_with: tensorboard\n",
      "\tproject_dir: output/\n",
      "\tnegatives_in_device: True\n",
      "\tpredict_from_last: 10000\n",
      "\teval_step: 1000\n",
      "\tfaiss:\n",
      "\t\tindex_type: Flat\n",
      "\t\tdimension: 768\n",
      "\t\tnprobe: 10000\n",
      "\t\tphrases_path: ../data_of_ReGPT/phrases_WikiText-103/phrases.npy\n",
      "\t\tmatrix_path: ../data_of_ReGPT/phrases_WikiText-103/phrases_embeddings.npy\n",
      "\tnegative_path: ../data_of_ReGPT/phrases_WikiText-103/negatives.tsv\n",
      "\tnegative_depth_in_pool: 101\n",
      "\tFNTP_threshold: 0.9\n",
      "\tmin_length: 0\n",
      "\tmax_length: 512\n",
      "\tdo_sample: False\n",
      "\ttop_k: 1\n",
      "\ttop_p: 1.0\n",
      "\tnegative_depth: 1\n",
      "\teos_token_id: 2\n",
      "dataset:\n",
      "\ttrain:\n",
      "\t\tdataset_name: ReGPTCorpusPretrainFromAfsDataset\n",
      "\t\tbatch_size: 8\n",
      "\t\tdata_name_or_path: ../data_of_ReGPT/c4_en/data-{:05d}-of-01658.arrow\n",
      "\t\tmax_seq_len: 512\n",
      "\t\tnegative_depth: 1\n",
      "\t\tfaiss:\n",
      "\t\t\tindex_type: Flat\n",
      "\t\t\tdimension: 768\n",
      "\t\t\tnprobe: 10000\n",
      "\t\t\tphrases_path: ../data_of_ReGPT/phrases_WikiText-103/phrases.npy\n",
      "\t\t\tmatrix_path: ../data_of_ReGPT/phrases_WikiText-103/phrases_embeddings.npy\n",
      "\t\tnegative_path: ../data_of_ReGPT/phrases_WikiText-103/negatives.tsv\n",
      "\t\tnegative_depth_in_pool: 101\n",
      "\t\tFNTP_threshold: 0.9\n",
      "\t\tpredict_from_last: 10000\n",
      "\ttest:\n",
      "\t\tdataset_name: ReGPTDocumentSummarizationSFTDataset\n",
      "\t\tbatch_size: 8\n",
      "\t\tdata_name_or_path: ../data_of_ReGPT/cnn_daily/\n",
      "\t\tmax_seq_len: 512\n",
      "\t\tnegative_depth: 1\n",
      "\t\tfaiss:\n",
      "\t\t\tindex_type: Flat\n",
      "\t\t\tdimension: 768\n",
      "\t\t\tnprobe: 10000\n",
      "\t\t\tphrases_path: ../data_of_ReGPT/phrases_WikiText-103/phrases.npy\n",
      "\t\t\tmatrix_path: ../data_of_ReGPT/phrases_WikiText-103/phrases_embeddings.npy\n",
      "\t\tnegative_path: ../data_of_ReGPT/phrases_WikiText-103/negatives.tsv\n",
      "\t\tnegative_depth_in_pool: 101\n",
      "\t\tFNTP_threshold: 0.9\n",
      "\t\tpredict_from_last: 10000\n",
      "generation_kwargs:\n",
      "\tmin_length: 0\n",
      "\tmax_length: 512\n",
      "\tdo_sample: False\n",
      "\ttop_k: 1\n",
      "\ttop_p: 1.0\n",
      "ReGPT_kwargs:\n",
      "\tfaiss:\n",
      "\t\tindex_type: Flat\n",
      "\t\tdimension: 768\n",
      "\t\tnprobe: 10000\n",
      "\t\tphrases_path: ../data_of_ReGPT/phrases_WikiText-103/phrases.npy\n",
      "\t\tmatrix_path: ../data_of_ReGPT/phrases_WikiText-103/phrases_embeddings.npy\n",
      "\tnegative_path: ../data_of_ReGPT/phrases_WikiText-103/negatives.tsv\n",
      "\tnegative_depth_in_pool: 101\n",
      "\tFNTP_threshold: 0.9\n",
      "Number of parameters: 6.6B\n",
      "Number of trainable parameters: 6476.271616M\n",
      "Ratio of trainable parameters: 98.02%\n",
      "<s> Beethoven melter is a combination of salt and iron powder mixed in with a mixture of salt and iron Griffith and voluminous . Woodmelter mixture is produced in a variety that is similar to traditional American iron or steelpan . Stripedmixed in mixture of salt and iron Griffith and voluminous . Wood melter mixture is mixed singly or in combinations before interchanging with from earlier traditional iron or calcite Formidarl. Wood Melter and Refiners are used to and Sebastian Fluké and other compounds for Boiling hydroformee \" salt and iron . Mistakemixed in the mixture combines with alkali and voluminOus. Wood Melter and Refiner is mostly used for cold storage as well as in a assortment such as with WatergRo and Waterbopod. Mistake mixed into salt and iron mixed in a variety that is similar to the classic American Iron . Mistakemixed into and Salt calcite FormidARL crystal .is mixed singly or blended together separately with alkali and voluminOUe to form several older Forms . Mistake the \n",
      "156.84690070152283\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from torch import Tensor, nn\n",
    "from dataset_factory import *\n",
    "import torch_optimizer as optim\n",
    "import torch.distributed as dist\n",
    "from dataclasses import dataclass\n",
    "from accelerate import Accelerator\n",
    "from transformers.file_utils import ModelOutput\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Optional, Tuple, Union, Dict\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "os.environ['http_proxy'] = 'http://172.19.57.45:3128'\n",
    "os.environ['https_proxy'] = 'http://172.19.57.45:3128'\n",
    "sys.path.append('/root/paddlejob/workspace/env_run/ReGPT')\n",
    "%cd /root/paddlejob/workspace/env_run/ReGPT\n",
    "config = get_config('config/rellama_config.yaml')\n",
    "ReGPT_kwargs = config['ReGPT_kwargs']\n",
    "config['training'].update(ReGPT_kwargs)\n",
    "config['dataset']['train'].update(ReGPT_kwargs)\n",
    "config['dataset']['test'].update(ReGPT_kwargs)\n",
    "generation_kwargs = config['generation_kwargs']\n",
    "config['training'].update(generation_kwargs)\n",
    "train_config = config['training']\n",
    "dataset_config = config['dataset']\n",
    "train_config['negative_depth'] = dataset_config['train']['negative_depth']\n",
    "dataset_config['train']['predict_from_last'] = train_config['predict_from_last']\n",
    "dataset_config['test']['predict_from_last'] = train_config['predict_from_last']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_config['tokenizer_name_or_path'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_config['eos_token_id'] = tokenizer.eos_token_id\n",
    "\n",
    "dataset_class = {\"DialogSFTDataset\": DialogSFTDataset, \"CorpusPretrainDataset\": CorpusPretrainDataset, \"ReGPTDialogSFTDataset\": ReGPTDialogSFTDataset, \"ReGPTCorpusPretrainDataset\": ReGPTCorpusPretrainDataset}\n",
    "\n",
    "config['training']['model_name_or_path'] = 'output/SFT-best/'\n",
    "print_args(config)\n",
    "model = ReGPTForCausalLM(train_config)\n",
    "# model.cuda()\n",
    "# model = model.half()\n",
    "\n",
    "\n",
    "def move_to_cuda(kwargs, device='cuda:0'):\n",
    "    for key in kwargs:\n",
    "        kwargs[key] = kwargs[key].to(device)\n",
    "def generate(text='Tsinghua University is a '):\n",
    "    text+=' '\n",
    "    kwargs = tokenizer([text],return_tensors='pt')\n",
    "#     move_to_cuda(kwargs)\n",
    "    output_ids = model.generate(**kwargs)\n",
    "#     print(tokenizer.convert_ids_to_tokens(output_ids.cpu().numpy().squeeze()))\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(output_ids.cpu().numpy().squeeze()))\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "model.train_config['do_sample'] = True\n",
    "model.train_config['top_k'] = 5\n",
    "model.train_config['top_p'] = 0.8\n",
    "model.train_config['max_length'] = 100\n",
    "print(generate('Beethoven'))\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c27076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Deathly Hallows revolves largely around hercynicualism revolving around the Life and Death of two young young boys who each disguise themselves as innocents – who are lured to Otherworld around the world by stealing their  Precious treasureand Cyril . They are initially lured to other world world by manipulating them of his past life transform into Otherworld . Much like the Otherworld itself is structured by HerméticPhilosophies as elaborated by and Cyril themselves in the process evolve into a sentient spirit world that is based on the true nature of Hermiten angel .and Cyril calculated for full advisories on tropical systems at 210 to 230ze and intensified to attain wind speeds of 210km  Hlimestone-basalt mount .and Cyril were briefly bombarded by otherworld 2011 , it degenerated into a  Storm of Zdeath and destruction shortly afterwards by Damián Logan and Jeremy Scott who left alongside him afterwards to embark on his new voyage across the Atlantic to continent. In hopes of finding success again on the brink of MannHepatine sees the return of one of his former Companions whose previous previous life has fallen into dis Himmel .and Cyril were briefly enveloped by Cataclysm . Seen \n"
     ]
    }
   ],
   "source": [
    "print(generate('Deathly Hallows'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62056a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Beijing is located in Far Eastern of China at BaiXiau County with roughly ⅓ square meter common areas with roughly 240 of Square metres and about 260 of Square mètre des s of two sides. Facing North China clockwise from North China parallel between Beijing and Hangzhouprovince , while running parallel to eastern China – between the Far Eastern peninsula and the far eastern peninsula of North China mount .Wangchuck mount .Chanchu mount .Wangchuck mount .Wangchuck mount .Kangchờng mount .Kangchóng mount .Kangchóng mount .Kangchông mount .Kangchóng mount .Chanchu mount .Kangchongmount .Kangchoa mount .Kangchông mount .Chanchu mount .Chochangmount .Kangchong mount .Kangchong . Same name as a  Mountainmount .Huachung , known also English as  MountCha Chünnu ASTA EHünnu Asta Ehunnü stä\n",
      "95.56739544868469\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model.train_config['do_sample'] = True\n",
    "model.train_config['top_k'] = 5\n",
    "model.train_config['top_p'] = 0.8\n",
    "model.train_config['max_length'] = 100\n",
    "print(generate('Beijing is located in'))\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0191568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Tsinghua bei  Yang BeiYang Banquo is famed for including being chosen as one of the five leading heroines of East China in 2008 by the International Business Times named as one of 500 best female figures in the world by the Daily Globe .com named WuBao as one of the top five heroines of 2010s in 2010 by Business Times was described as the youngest person ever to win the Best Female Performance by a Female figure in the Worldlands of and South American Capistr O. Wu Tepisode , while praising her character and fearlessness in tandem with her fellow heroines garnered widespread acclaim from both critics and media critics . Upon its release , it  garnered widespread acclaim from critics and fans across both the mass media and mass media for both its portrayal and originality regards to its portrayal and Theatricality and Cyril Monteith were praised for their portrayal and artistry especially in regards to the fates of several older female characters in present day china . TE SingHua Bei Yang Bei Yang bei yang banquo was named as one of the 50 Most admired heroines of 21st century in 2009 by Britannia . Wu Bao was named the best Performance by a Female in the leading film genre in the history of the show by \n",
      "88.51301860809326\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model.train_config['do_sample'] = True\n",
    "model.train_config['top_k'] = 5\n",
    "model.train_config['top_p'] = 0.8\n",
    "model.train_config['max_length'] = 100\n",
    "print(generate('Tsinghua'))\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4424a7f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Beijing City University bei Yang International University Bei Yang BeiYang International University bei and Hebei City University of bei  Yang Bei  Yang International University BeileYang International University West Point , ny. University BeiYangInternational University BeiYang BeiYangInternational University bei  Yang BeiYang i International BeiYang i International Bei yang International University bei yang i International Bei Yang international Bei rong i International Bei rong International Bei ronji IUgolinóInternational University BeiRonginternational bei rong i International Bei Rön JiUgolINOWorld University Bei Ron ji IUgolíno International University BeirongInternational Beirong international BeiRong i International bei Ron Ji Ugolíno World University bei ron JiUgolINOWorld University Bei \n",
      "88.25420546531677\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model.train_config['do_sample'] = True\n",
    "model.train_config['top_k'] = 5\n",
    "model.train_config['top_p'] = 0.8\n",
    "model.train_config['max_length'] = 100\n",
    "print(generate('Beijing'))\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feed9173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Young lady loves me Finally Parting with her child and second son loves .loving Family and  Friendship continues for more years until after she marries Annabel Finch . Elle soon becomes infatuated with her girlfriend and he had to leave her home for religious reasons for many years before Marrying Him . She again pursues  Himmlers who is now happily married again with his late mother Annabel  Finch. Betsy soon becomes romantically interested in Himmlers and soon becomes romantically involved with her stepbrother  Priscilla  Finch. Elle soon discovers that she is romantically interested in HimmLerloves .designation is given to her by & Mrs. Greene whom she originally met again during a fracas together while Annabel Joins Himmlerin her husband 's personal union . Ellen is initially reluctant design but hesitates again when she makes amends again when she makes plans to Marry  Himmtitanium , thus ending both her marriage and eventuality she is to revert to the previous life . She again makes plans to   Marry HimmLER instead , but makes plans to move back back to the United States . The next month as she is still recovering from illness\n",
      "90.0628890991211\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model.train_config['do_sample'] = True\n",
    "model.train_config['top_k'] = 5\n",
    "model.train_config['top_p'] = 0.8\n",
    "model.train_config['max_length'] = 100\n",
    "print(generate('Young lady loves me'))\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcdd9b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Tell me a story about China . Leaving Home Front at home with family friend s in Portland , MainTheodore , Shelby lays Eyes Wide Shut  While on the road crew – and he is soon drawn back to Home Front while dealing with traumatic stress injuries suffered while returning from a boat trip . Back home ,  Shelby is soon welcomed back home by late friend  Priscilla Massey , whose daughter is being killed in 1918 flu the next month while at a work camp in Ben ItoTheodore , Shelby is soon welcomed back at home by family friend s in western  Willamette county . Back home , Priscilla sits and sleeps with of his friends again at Greenwood Memorial Church in Cedar bluff, where late mother  Priscilla 20 feet from her Honeymoon . Back home ,  Priscilla makes plans to return home as soon as she makes plans to Leave Home the next month alone after home with her late mother leaving .however she makes amends with her when she makes amends with her to former friend  Priscilla ascent , who now hate you again for losing him again due to nervous breakdown that resulted from herculeanHerculiaising with him while in a work camp in Ben Ito Theodore , \n",
      "90.01342105865479\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model.train_config['do_sample'] = True\n",
    "model.train_config['top_k'] = 5\n",
    "model.train_config['top_p'] = 0.8\n",
    "model.train_config['max_length'] = 100\n",
    "print(generate('Tell me a story about China'))\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23571a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
