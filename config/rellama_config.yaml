trainer: ReGPTLanguageModelTrainer
training:
  project_name: llama2-7b-sft
  # model_name_or_path: ../epoch-329/SFT-new/
  model_name_or_path: ../LLAMA-2-7B
  lora_model_name_or_path: output/SFT-new/
  # tokenizer_name_or_path: ../data_of_ReGPT/llama2-7b-phrase-tokenizer-trained-on-WikiText103/
  tokenizer_name_or_path: ../LLAMA-2-7B
  num_layers_unfrozen: -1
  gradient_checkpointing: true
  optimizer: 
    name: Lamb
    kwargs:
      lr: 5e-4
      weight_decay: 0.0
      betas: [0.9, 0.999]
      eps: 1e-08
  scheduler:
    name: LinearLR
    kwargs:
      total_iters: 100
    #   # T_max: 1000
    #   # eta_min: 3e-5
  num_epochs: 1000
  log_with: tensorboard
  project_dir: output/
  negatives_in_device: true
  predict_from_last: 10000
  eval_step: 1000
  start_from: 2

dataset:
  train:
    dataset_name: ReGPTCorpusPretrainFromAfsDataset
    batch_size: 32
    data_name_or_path: ../data_of_ReGPT/En-Wiki/sorted_datasets_train/data-{:05d}-of-00030.arrow
    max_seq_len: 512
    negative_depth: 1
  test:
    dataset_name: ReGPTDocumentSummarizationSFTDataset
    batch_size: 32
    data_name_or_path: ../data_of_ReGPT/cnn_daily/
    max_seq_len: 512
    negative_depth: 1

generation_kwargs:
  min_length: 0
  max_length: 512
  do_sample: false
  top_k: 1
  top_p: 1.0

ReGPT_kwargs:
  faiss:
    index_type: Flat
    dimension: 4096
    nprobe: 10000
    phrases_path: ../data_of_ReGPT/phrases_original_repllama/phrases.npy
    matrix_path: ../data_of_ReGPT/phrases_original_repllama/phrases_embeddings.npy
  negative_path: ../data_of_ReGPT/phrases_original_repllama/negatives.tsv
  negative_depth_in_pool: 101
  FNTP_threshold: 0.9
